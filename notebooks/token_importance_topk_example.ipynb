{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": "# Token Importance Top-k Example"},
  {"cell_type": "code", "metadata": {}, "source": "from tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom neural_feature_importance.embedding_callbacks import EmbeddingVarianceImportanceKeras\nfrom token_topk_utils import analyze_samples, build_model\n\nMAX_FEATURES = 5000\nMAX_LEN = 400\nTOP_K = 5\nNUM_SAMPLES = 5\n\n(x_train, y_train), _ = imdb.load_data(num_words=MAX_FEATURES)\nx_train = pad_sequences(x_train, maxlen=MAX_LEN)\n\nmodel = build_model()\ncallback = EmbeddingVarianceImportanceKeras()\nmodel.fit(x_train, y_train, epochs=2, batch_size=128, callbacks=[callback], verbose=0)\n\nscores = callback.feature_importances_\nword_index = imdb.get_word_index()\nindex_word = {v + 3: k for k, v in word_index.items()}\nindex_word[0] = \"<PAD>\"\nindex_word[1] = \"<START>\"\nindex_word[2] = \"<UNK>\"\nindex_word[3] = \"<UNUSED>\"\n\nanalyze_samples(x_train, y_train, scores, index_word, NUM_SAMPLES, TOP_K)"}
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
