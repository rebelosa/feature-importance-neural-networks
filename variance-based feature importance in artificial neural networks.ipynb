{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import np_utils, to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers.advanced_activations import PReLU\n",
    "from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "from tensorflow.keras.utils import np_utils\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN1(input_dim, output_dim, isClassification = True):\n",
    "    print(\"Starting NN1\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=input_dim, activation='linear', kernel_initializer='normal', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(100, activation='linear', kernel_initializer='normal', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(50, activation='linear', kernel_initializer='normal', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "    if (isClassification == False):\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    elif (isClassification == True):\n",
    "        model.add(Dense(output_dim, activation='softmax', kernel_initializer='normal'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN2(input_dim, output_dim, isClassification = True):\n",
    "    print(\"Starting NN2\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=input_dim, activation='relu', kernel_initializer='normal', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='normal', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(50, activation='relu', kernel_initializer='normal', kernel_regularizer=l2(0.01)))\n",
    "        \n",
    "    if (isClassification == False):\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    elif (isClassification == True):\n",
    "        model.add(Dense(output_dim, activation='softmax', kernel_initializer='normal'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Model\n",
    "def DeepNN(input_dim, output_dim, isClassification = True):\n",
    "    print(\"Starting DeepNN\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=input_dim, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2048, kernel_initializer='normal', kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, kernel_initializer='random_uniform', kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2048, kernel_initializer='random_uniform', kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, kernel_initializer='normal', kernel_regularizer=l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(500, kernel_initializer='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(PReLU())\n",
    "\n",
    "    if (isClassification == False):\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    elif (isClassification == True):\n",
    "        model.add(Dense(output_dim, activation='softmax', kernel_initializer='normal'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Importance methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_importance import VarianceImportanceCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://csiu.github.io/blog/update/2017/03/29/day33.html\n",
    "def garson(A, B):\n",
    "    \"\"\"\n",
    "    Computes Garson's algorithm\n",
    "    A = matrix of weights of input-hidden layer (rows=input & cols=hidden)\n",
    "    B = vector of weights of hidden-output layer\n",
    "    \"\"\"\n",
    "    B = np.diag(B)\n",
    "\n",
    "    # connection weight through the different hidden node\n",
    "    cw = np.dot(A, B)\n",
    "\n",
    "    # weight through node (axis=0 is column; sum per input feature)\n",
    "    cw_h = abs(cw).sum(axis=0)\n",
    "\n",
    "    # relative contribution of input neuron to outgoing signal of each hidden neuron\n",
    "    # sum to find relative contribution of input neuron\n",
    "    rc = np.divide(abs(cw), abs(cw_h))\n",
    "    rc = rc.sum(axis=1)\n",
    "\n",
    "    # normalize to 100% for relative importance\n",
    "    ri = rc / rc.sum()\n",
    "    return(ri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://csiu.github.io/blog/update/2017/03/29/day33.html\n",
    "class VarImpGarson(tensorflow.keras.callbacks.Callback):\n",
    "    def __init__(self, verbose=0):\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def on_train_end(self, batch, logs={}):\n",
    "        if self.verbose:\n",
    "            print(\"VarImp Garson\")\n",
    "        \"\"\"\n",
    "        Computes Garson's algorithm\n",
    "        A = matrix of weights of input-hidden layer (rows=input & cols=hidden)\n",
    "        B = vector of weights of hidden-output layer\n",
    "        \"\"\"\n",
    "        A = self.model.layers[0].get_weights()[0]\n",
    "        B = self.model.layers[len(self.model.layers)-1].get_weights()[0]\n",
    "        \n",
    "        self.var_scores = 0\n",
    "        for i in range(B.shape[1]):\n",
    "            self.var_scores += garson(A, np.transpose(B)[i])\n",
    "        if self.verbose:\n",
    "            print(\"Most important variables: \",\n",
    "                np.array(self.var_scores).argsort()[-10:][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-One-Feature-Out LOFO\n",
    "def LeaveOneFeatureOut(model, X, Y):\n",
    "    OneOutScore = []\n",
    "    n = X.shape[0]\n",
    "    for i in range(0,X.shape[1]):\n",
    "        newX = X.copy()\n",
    "        newX[:,i] = 0 #np.random.normal(0,1,n)\n",
    "        OneOutScore.append(model.evaluate(newX, Y, batch_size=2048, verbose=0))\n",
    "    OneOutScore = pd.DataFrame(OneOutScore[:])\n",
    "    ordered = np.argsort(-OneOutScore.iloc[:,0])\n",
    "    return(OneOutScore, ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing variable importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings obtained for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "data.append({\"name\": 'breastcancer', \"classification\": True, \"data\": datasets.load_breast_cancer()})\n",
    "data.append({\"name\": 'digits', \"classification\": True, \"data\": datasets.load_digits()})\n",
    "data.append({\"name\": 'iris', \"classification\": True, \"data\": datasets.load_iris()})\n",
    "data.append({\"name\": 'wine', \"classification\": True, \"data\": datasets.load_wine()})\n",
    "data.append({\"name\": 'boston', \"classification\": False, \"data\": datasets.load_boston()})\n",
    "data.append({\"name\": 'diabetes', \"classification\": False, \"data\": datasets.load_diabetes()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "class AccuracyMonitor(Callback):\n",
    "    def __init__(self,\n",
    "                 monitor='val_acc',\n",
    "                 verbose=0,\n",
    "                 min_epochs=5,\n",
    "                 baseline=None):\n",
    "        super(AccuracyMonitor, self).__init__()\n",
    "\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "        self.verbose = verbose\n",
    "        self.min_epochs = min_epochs\n",
    "        self.stopped_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get(self.monitor) > self.baseline and epoch > self.min_epochs:\n",
    "            self.stopped_epoch = epoch\n",
    "            self.model.stop_training = True\n",
    "            print('\\n Stopped at epoch {epoch}. Accuracy of {accuracy} reached.'.format(epoch=(self.stopped_epoch + 1), accuracy=logs.get(self.monitor)), \"\\n\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def runExp(data, mdl = \"NN1\", xseed = 42, epochs = 1000, verbose = 0):\n",
    "\n",
    "    res = list()\n",
    "    VIANN_list = []\n",
    "    Garson_list = []\n",
    "    LOFO_list = []\n",
    "    RF_list = []\n",
    "    for i in range(len(data)):\n",
    "        seed(xseed)\n",
    "        \n",
    "        dataset = data[i]['data']\n",
    "        isClassification = data[i]['classification']\n",
    "        datname = data[i]['name']\n",
    "        \n",
    "        print(\"============\")\n",
    "        print( data[i]['name'])\n",
    "        print(\"============\\n\")\n",
    "\n",
    "        if isClassification == True:\n",
    "            #Classification\n",
    "\n",
    "            labels_encoded = []\n",
    "            for labels in [dataset.target]:\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(labels)\n",
    "                encoded_Y = encoder.transform(labels)\n",
    "                # convert integers to dummy variables (i.e. one hot encoded)\n",
    "                labels_encoded.append(np_utils.to_categorical(encoded_Y))\n",
    "            dataset.targetLabels = labels_encoded[0]\n",
    "\n",
    "            # fit a Random Forest model to the data\n",
    "            RFmodel = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "            output_size = dataset.targetLabels.shape[1]\n",
    "\n",
    "        else:\n",
    "            dataset.targetLabels = scale(dataset.target)\n",
    "            output_size = 1\n",
    "\n",
    "            # fit a Random Forest model to the data\n",
    "            RFmodel = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "        X = scale(dataset.data)\n",
    "        Y = dataset.targetLabels\n",
    "\n",
    "        RFmodel.fit(X, Y)\n",
    "        \n",
    "        VIANN = VarianceImportanceCallback()\n",
    "        Garson = VarImpGarson(verbose=verbose)\n",
    "\n",
    "        if (mdl == \"NN1\"):\n",
    "            model = NN1(X.shape[1], output_size, isClassification)\n",
    "        elif (mdl == \"NN2\"):\n",
    "            model = NN2(X.shape[1], output_size, isClassification)\n",
    "        elif (mdl == \"DeepNN\"):\n",
    "            model = DeepNN(X.shape[1], output_size, isClassification)\n",
    "        \n",
    "        clbs = [VIANN,Garson]\n",
    "        if isClassification == True:\n",
    "            clbs.append(AccuracyMonitor(monitor='val_acc', baseline=0.95, min_epochs = 5))\n",
    "        else:\n",
    "            epochs = 100\n",
    "        \n",
    "        model.fit(X, Y, validation_split=0.05, epochs=epochs, batch_size=np.round(X.shape[0]/7).astype(int), shuffle=True, \n",
    "                  verbose=verbose, callbacks = clbs)\n",
    "\n",
    "        LOFO, LOFO_Ordered = LeaveOneFeatureOut(model, X, Y)\n",
    "\n",
    "        print('VIANN vs LOFO:  ', round(np.corrcoef([VIANN.var_scores,LOFO[0]])[0,1], 2))\n",
    "        print('VIANN vs RF:    ', round(np.corrcoef([VIANN.var_scores,RFmodel.feature_importances_])[0,1], 2))\n",
    "        print('Garson vs LOFO: ', round(np.corrcoef([Garson.var_scores,LOFO[0]])[0,1], 2))\n",
    "        print('Garson vs VIANN:', round(np.corrcoef([Garson.var_scores,VIANN.var_scores])[0,1], 2))\n",
    "        \n",
    "        res.append([data[i]['name'],\n",
    "                    round(np.corrcoef([VIANN.var_scores,LOFO[0]])[0,1], 2), \n",
    "                    round(np.corrcoef([VIANN.var_scores,RFmodel.feature_importances_])[0,1], 2),\n",
    "                    round(np.corrcoef([Garson.var_scores,LOFO[0]])[0,1], 2),\n",
    "                    round(np.corrcoef([Garson.var_scores,VIANN.var_scores])[0,1], 2)\n",
    "                          ])\n",
    "        \n",
    "        VIANN_list.append([data[i]['name'], VIANN.var_scores])\n",
    "        Garson_list.append([data[i]['name'], Garson.var_scores])\n",
    "        LOFO_list.append([data[i]['name'], LOFO])\n",
    "        RF_list.append([data[i]['name'], RFmodel.feature_importances_])\n",
    "        \n",
    "    df = pd.DataFrame(res)\n",
    "    df.columns = (\"Dataset\", \"VIANN vs LOFO\", \"VIANN vs RF\", \"Garson vs LOFO\", \"Garson vs VIANN\")\n",
    "    \n",
    "    return df, VIANN_list, Garson_list, LOFO_list, RF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsNN1, VIANN_NN1, Garson_NN1, LOFO_NN1, RF = runExp(data, mdl = \"NN1\", verbose = 0)\n",
    "rsNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsNN2, VIANN_NN2, Garson_NN2, LOFO_NN2, RF = runExp(data, mdl = \"NN2\", verbose = 0)\n",
    "rsNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsDeepNN, VIANN_DeepNN, Garson_DeepNN, LOFO_DeepNN, RF = runExp(data, mdl = \"DeepNN\", verbose = 0)\n",
    "rsDeepNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results published in Discovery Science 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsDeepNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"NN2\"\n",
    "datname = VIANN_NN2[1][0]\n",
    "xx = VIANN_NN2[1][1]\n",
    "yy = LOFO_NN2[1][1][0]\n",
    "\n",
    "f = plt.figure()\n",
    "plt.scatter(xx, yy)\n",
    "plt.xlabel('VIANN')\n",
    "plt.ylabel('LOFO')\n",
    "plt.title('VIANN vs LOFO' + \" (\" + datname + \" dataset)\")\n",
    "plt.show()\n",
    "f.savefig(\"VIANNvsLOFO_\" + datname + \"_\" + modelname +\".pdf\")\n",
    "\n",
    "print(np.corrcoef([xx,yy])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"DeepNN\"\n",
    "datname = VIANN_DeepNN[1][0]\n",
    "xx = VIANN_DeepNN[1][1]\n",
    "yy = RF[1][1]\n",
    "\n",
    "f = plt.figure()\n",
    "plt.scatter(xx, yy)\n",
    "plt.xlabel('VIANN')\n",
    "plt.ylabel('RF feature importance')\n",
    "plt.title('VIANN vs RF' + \" (\" + datname + \" dataset)\")\n",
    "plt.show()\n",
    "f.savefig(\"VIANNvsRF_\" + datname + \"_\" + modelname +\".pdf\")\n",
    "\n",
    "print(np.corrcoef([xx,yy])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
